# -*- coding: utf-8 -*-
"""Behavioral Cloning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_Xkb_cwC54drjM7JO_G70aJtMfhYOzt
"""

!git clone https://github.com/akhildesh93/Self-Driving-Car-Machine-Learning.git

!ls Track

!pip3 install imgaug

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import keras
from keras.models import Sequential
from keras.layers import Dense, Convolution2D, MaxPooling2D, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import cv2
import pandas as pd
import random
import os
import ntpath
from imgaug import augmenters as iaa

np.random.seed(0)

datadir = 'Track'
  #columns of data
columns = ['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']
  #reading csv and placing into data
data = pd.read_csv(os.path.join(datadir, 'driving_log.csv'), names=columns)
  #show some data
data.head()

#removes computer path for images
def path_leaf(path):
  head, tail = ntpath.split(path)
  return tail

data['center'] = data['center'].apply(path_leaf)
data['left'] = data['left'].apply(path_leaf)
data['right'] = data['right'].apply(path_leaf)
data.head()

num_bins = 25
samples_per_bin = 400 #limits data from being too skewed
hist, bins = np.histogram(data['steering'], num_bins) #divides data into num of bins
center = (bins[:-1]+bins[1:]) * 0.5 #centers data
plt.bar(center, hist, width=0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])), (samples_per_bin, samples_per_bin))

#balancing data
remove_list = []
for j in range(num_bins):
  list_ = []
  for i in range(len(data['steering'])):
    if data['steering'][i] >= bins[j] and data['steering'][i] <= bins[j+1]:
      list_.append(i)
  list_ = shuffle(list_) #rearranges to preserve info for all parts of track
  list_ = list_[samples_per_bin:] #removes above threshold
  remove_list.extend(list_)

print('total data: ', len(data))
data.drop(data.index[remove_list], inplace=True)
print('remaining: ', len(data))
print('removed: ', len(remove_list))

hist, _ = np.histogram(data['steering'], num_bins)
plt.bar(center, hist, width=0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])), (samples_per_bin, samples_per_bin))

#load steering data
print(data.iloc[1])
def load_img_steering(datadir, df):
  image_path = []
  steering = []
  for i in range(len(data)):
    indexed_data = data.iloc[i]
    center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]
      #strip removes spaces in string
    image_path.append(os.path.join(datadir, center.strip()))
    steering.append(indexed_data[3])
  #put into np arrays
  image_paths = np.asarray(image_path)
  steerings = np.asarray(steering)
  return image_paths, steerings


image_paths, steerings = load_img_steering(datadir + '/IMG', data)

X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=6)
print("training samples: {}\nValid Samples: {}".format(len(X_train), len(X_valid)))

fig, axes = plt.subplots(1, 2, figsize=(12,4))
axes[0].hist(y_train, bins=num_bins, width=0.05, color='blue')
axes[0].set_title("training set")
axes[1].hist(y_valid, bins=num_bins, width=0.05, color='red')
axes[1].set_title("validation set")

#zooming data augmentation
def zoom(image):
    #zoom up to 30%
  zoom = iaa.Affine(scale=(1,1.3))
    #apply to image and augment
  image = zoom.augment_image(image)
  return image

image = image_paths[random.randint(0,1000)]
original_image = mpimg.imread(image)
zoomed_image = zoom(original_image)

fig, axs = plt.subplots(1,2,figsize=(15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title("original image")
axs[1].imshow(zoomed_image)
axs[1].set_title("zoomed image")

#pan images data augmentation
def pan(image):
  pan = iaa.Affine(translate_percent={"x":(-0.1,0.1), "y":(-0.1,0.1)})
  image = pan.augment_image(image)
  return image

image = image_paths[random.randint(0,1000)]
original_image = mpimg.imread(image)
panned_image = pan(original_image)

fig, axs = plt.subplots(1,2,figsize=(15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title("original image")
axs[1].imshow(panned_image)
axs[1].set_title("panned image")

#brightness data augmentation
def img_random_brightness(image):
  brightness = iaa.Multiply((0.2,1.2))
  image = brightness.augment_image(image)
  return image

image = image_paths[random.randint(0,1000)]
original_image = mpimg.imread(image)
brightened_image = img_random_brightness(original_image)

fig, axs = plt.subplots(1,2,figsize=(15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title("original image")
axs[1].imshow(brightened_image)
axs[1].set_title("brightness altered image")

#flipping data augmentation
def img_random_flip(image, steering_angle):
    #horizontal flip
  image = cv2.flip(image, 1)
  steering_angle = -1*steering_angle
  return image, steering_angle

random_index = random.randint(0,1000)
image = image_paths[random_index]
steering_angle = steerings[random_index]

original_image = mpimg.imread(image)
flipped_image, flipped_steering_angle = img_random_flip(original_image, steering_angle)

fig, axs = plt.subplots(1,2,figsize=(15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title("original image - " + 'steering angle: '+str(steering_angle))
axs[1].imshow(flipped_image)
axs[1].set_title("flipped  image - " + 'steering angle: '+str(flipped_steering_angle))

#randomly augments data
def random_augment(image, steering_angle):
  image = mpimg.imread(image)
  #50% chance
  if np.random.rand() < 0.5:
    image = pan(image)
  if np.random.rand() < 0.5:
    image = zoom(image)
  if np.random.rand() < 0.5:
    image = img_random_brightness(image)
  if np.random.rand() < 0.5:
    image, steering_angle = img_random_flip(image, steering_angle)
  return image, steering_angle

ncol = 2
nrow = 10
fig, axs = plt.subplots(nrow, ncol, figsize=(15,50))
fig.tight_layout()

for i in range(10):
  randnum = random.randint(0,len(image_paths)-1)
  random_image = image_paths[randnum]
  random_steering = steerings[randnum]

  original_image = mpimg.imread(random_image)
  augmented_image, steering = random_augment(random_image, random_steering)

  axs[i][0].imshow(original_image)
  axs[i][0].set_title("original image")
  axs[i][1].imshow(augmented_image)
  axs[i][1].set_title("original image")

#preprocessing images
def img_preprocess(img):
  #read image path and store image
  img = img[60:135,:,:]
  img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV) #colorspace good for nvidia model
  img = cv2.GaussianBlur(img, (3,3), 0) #gaussian blur
  img = cv2.resize(img, (200,66)) #resize 
  img = img/255 #normalize
  return img

image = image_paths[100]
original_image = mpimg.imread(image)
preprocessed_image = img_preprocess(original_image)

fig, axs = plt.subplots(1,2, figsize=(15,10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title("original image")
axs[1].imshow(preprocessed_image)
axs[1].set_title("preprocessed image")

#batch generator, saves memory as only used when required
def batch_generator(image_paths, steering_ang, batch_size, istraining):
  while True:
    batch_img = []
    batch_steering = []
    for i in range(batch_size):
      random_index = random.randint(0,len(image_paths)-1)
      if istraining:
        im, steering = random_augment(image_paths[random_index], steering_ang[random_index])
      else:
        im = mpimg.imread(image_paths[random_index])
        steering = steering_ang[random_index]

      im = img_preprocess(im)
      batch_img.append(im)
      batch_steering.append(steering)
    yield (np.asarray(batch_img), np.asarray(batch_steering))

X_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))
X_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))

fig, axs = plt.subplots(1,2,figsize=(15,10))
fig.tight_layout()
axs[0].imshow(X_train_gen[0])
axs[0].set_title("training image - " + 'steering angle: '+str(steering_angle))
axs[1].imshow(X_valid_gen[0])
axs[1].set_title("validation  image - " + 'steering angle: '+str(flipped_steering_angle))

#create nvidia convolutional model
def nvidia_model():
  model = Sequential()
  #elu has nonzero gradient in negative, so can fix weight values in backpropagating,
  #where relu can cause a node to "die" with a weight of 0
   
    #24 filters, kernel 5x5, strides are 2 pixels down and to the side
  model.add(Convolution2D(24, kernel_size=(5,5), strides=(2,2), input_shape=(66,200,3), activation='elu'))
  model.add(Convolution2D(36, kernel_size=(5,5), strides=(2,2), activation='elu'))
  model.add(Convolution2D(48, kernel_size=(5,5), strides=(2,2), activation='elu'))
  model.add(Convolution2D(64, kernel_size=(3,3), activation='elu'))
  model.add(Convolution2D(64, kernel_size=(3,3), activation='elu'))
  #model.add(Dropout(0.5))
  
  model.add(Flatten())

  model.add(Dense(100, activation='elu'))
  #model.add(Dropout(0.5))

  model.add(Dense(50, activation='elu'))
  #model.add(Dropout(0.5))

  model.add(Dense(10, activation='elu'))
  #model.add(Dropout(0.5))
  
    #output layer
  model.add(Dense(1))

  model.compile(loss='mse', optimizer=Adam(learning_rate=1e-3))
  return model

model = nvidia_model()
model.summary()

history = model.fit_generator(batch_generator(X_train, y_train, 100, 1),
                                  steps_per_epoch=300, 
                                  epochs=10,
                                  validation_data=batch_generator(X_valid, y_valid, 100, 0),
                                  validation_steps=200,
                                  verbose=1,
                                  shuffle = 1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('loss')
plt.xlabel('epoch')

model.save('model.h5')

from google.colab import files
files.download('model.h5')